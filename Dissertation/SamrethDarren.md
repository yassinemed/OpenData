#  Interopérabilité et Data Science

## C'est quoi l'interopérabilité ?

Nous connaissons aujourd'hui une augmentation de la donnée collectée qui révolutionne le monde dans lequel nous vivons. En effet, d'après IBM, il a été crée ces deux dernières années 90% de la donnée totale. D'ici 2020, Intel prévoit que 200 milliards d'objets connectés seront en circulation dans le monde, et l'IDC prévoit que 203 milliards de dollars seront générés par l'exploitation de la donnée. Afin de pouvoir utiliser tout le potentiel de cette masse de donnée, il serait nécessaire que cette masse puisse circuler librement et sans altération entre tous les acteurs, et donc que les différents systèmes puissent fonctionner entre eux, sans avoir besoin de développer une solution supplémentaire pour s'assurer de leur bonne communication : c'est ce que l'on appelle l'interopérabilité. Même si elle s'apparente aujourd'hui à une chimère, il est quand même intéressant de se pencher sur les atouts que peut apporter cette organisation, notamment pour ceux qui cherchent à exploiter cette donnée. Ainsi, nous allons voir dans un premier temps un exemple de bonne interopérabilité : le web. Puis, nous verrons ce qu'apporte l'interopérabilité du web à un data scientist. Nous conclurons ensuite sur une perspective de nos carrières.

## Pourquoi le Web est-il un bon exemple d'interopérabilité ?

S'il fallait choisir un exemple pour illustrer l'interopérabilité, le Web serait une bonne illustration. En effet, si internet contient plusieurs milliards de site, il réussit l'exploit de ne nécessiter uniquement d'un navigateur internet pour pouvoir ouvrir, avec parfois plus ou moins de réussite, n'importe quelle page internet. Ceci est possible grâce aux normes mises en place par le World Wide Consortium (W3C), qui assure « *un seul web partout et pour tous* ». Ils sont ainsi garants de la standardisation de plusieurs technologies comme le HTML, le XML ou encore le SPARQL. De plus, le HTTP est le seul protocole de communication qui permet à plusieurs logiciels d'interagir ensemble. En effet, le HTTP permet aux clients (et donc aux navigateurs internet) d'envoyer une demande au serveur, qui va à son tour envoyer vers le client les fichiers requis pour assembler la page web. En plus de cela, le HTTP permet aussi de connecter plusieurs fichiers entre eux (= hypertexte), afin de passer aisément d'une page web à une autre. Internet étant maintenant devenu un élément essentiel de communication, la transmission de l'information, et plus généralement la transmission de la donnée, est très largement facilité par le web. L'initiative Open-Data, qui incite aux acteurs à « *ouvrir* » leurs données (c'est-à-dire à les rendre accessibles sans restriction légale et technologique), est un exemple de l'effort mis en place pour l'interopérabilité des données, bien que seulement 8% des collectivités territoriales respectent la loi sur l'Open-Data entrée en vigueur le 7 octobre dernier. Or, l'Open-Data et l'interopérabilité du web en général est un outil clé pour ceux qui s'intéressent aux données, et plus spécifiquement aux data scientist.

## Qu’est-ce que peut apporter l’interopérabilité sur le web à un data scientist ?

Le data scientist doit monter dans trois compétences : en expérience métier, afin de pouvoir se positionner dans les décisions liées au secteur d'activité ; en mathématiques, et plus spécifiquement en statistiques, afin de pouvoir mettre en place des modèles, les utiliser et les évaluer correctement ; et en informatique, afin d'être non seulement capable d'implémenter les modèles, mais aussi et surtout, être capable de récolter, et nettoyer les données nécessaires. Ces deux derniers points sont les tâches qui prennent le plus de temps au data scientist, car sans des données propres et traitées correctement, impossible de mettre en place un modèle pertinent. Mais surtout, les données auxquelles est confronté le data scientist sont souvent déstructurées et hétérogènes, et peuvent donc nécessiter de nombreux traitements afin d'être exploitables. C'est pour cela que l'interopérabilité en général est un gain de temps considérable pour le data scientist. De plus, comme précisé en introduction, l'exploitation de l'explosion du volume des données de ces dernières années est un enjeu capital. Si la multiplication des objets connectés en est un facteur certain, le web reste le plus grand fournisseur de donnée, de par sa place dans la transmission de l'information, mais aussi et surtout pour son contenu (largement influencé aujourd'hui par l'ampleur des réseaux sociaux) gigantesque. Un data scientist doit ainsi pouvoir être capable de récupérer de la donnée depuis internet. Or, le web étant un environnement extrêmement divers et varié, contenant des milliards de pages organisées et élaborées différemment, une interopérabilité du web lui permet de récupérer de façon efficace et efficiente les données qu'il recherche, et ainsi lui faire gagner un temps considérable et de l'exactitude dans ses modèles grâce aux données qui peuvent être en quantité et provenir de sources variées.

## Perspectives sur vos carrières

Dans ce contexte d'émergence de la donnée et du Big Data, le data scientist doit être capable de manipuler des masses de données considérables. De plus, il doit pouvoir apporter et connecter son entreprise à la connaissance issus des données produites dans le monde extérieur. Ainsi, l'interopérabilité des systèmes d'information est un point capital, et le web, en plus d'être un outil qui connaît une forte culture de l'interopérabilité, et une véritable mine de donnée. Pour avoir déjà connu des situations où le client apportait ses propres données, mais qui nécessitaient avant d'être exploité un fastidieux travail de nettoyage et de parsing, il est évident que l'exploitation de l'interopérabilité du web, et plus généralement l'interopérabilité de la donnée est une opportunité à saisir, pour peu que les différents acteurs trouvent un compromis pour rendre cette solution envisageable.