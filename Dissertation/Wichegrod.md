# Interopérabilité et Data Science

L’interopérabilité peut être définie comme la capacité d’un d’un système à fonctionner avec d’autres systèmes sans restriction d’accès ou de mise en œuvre. Elle a pour but de faciliter, au moyen de normes et de standards, les interactions entre des entités différentes. Un exemple est celui des téléphones : il est possible de téléphoner simplement sans que les appareils soient de la même marque ou fournis par le même opérateur, et ce grâce à des normes régissant ce domaine.

En informatique, l’interopérabilité est rendue possible par des interfaces entre les outils qui respectent plus ou moins les normes préétablies.

Le world-wide web, ou « toile d’araignées mondiale », est bien ce qu’elle dit être : mondiale. Le web, grâce à internet, a ouvert à tous la communication avec le reste du monde. Et puisque M. Tim Berners-Lee fait les choses bien, l’interopérabilité semblait déjà être l’idéal à atteindre.

Le web permettant à tout un chacun de donner et de recevoir, encore faut-il parler la même langue. Et non pas celle des utilisateurs (pour permettre aux utilisateurs du web de tous parler la même langue, se référer au « Web 3.0 », quoi que ça veuille dire), non, il s’agit ici des systèmes, des applications qui doivent avoir des protocoles communs. Le meilleur exemple est celui du protocole HTTP, la base de l’interopérabilité du web dans son ensemble.

D’autre part, l’interopérabilité est rendue possible par des normes et des standards. Certains organismes, comme le World-Wide Web Consortium (W3C), prônent la rédaction de normes collaborative : des entreprises ou des individus peuvent participer à l’élaboration de normes dont les documentations seront ouvertes à tous. En ont découlé les standards tels que XML, HTML et CSS.

Autant de standards à respecter, autant d’avantages pour les autres utilisateurs du web. Sans standards, comment la communauté scientifique échangerait-elle des articles ou des données sur le web ? Sans standards, comment le CAC 40 marcherait-il alors que personne n’y comprend rien, même avec les standards ?

Il existe également des acteurs dont le poids propre leur permet de créer des formats à des fins lucratives car ils en détiennent l’exclusivité. Mais les organismes mentionnés plus haut, soutenus par des développeurs « citoyens », tentent d’ouvrir le web autant que possible.

C’est par exemple grâce à ce genre d’initiative qu’existent les obligations pour les  administrations de publier des données récoltées sous forme d’open data. L’administration publique étant ce qu’elle est, il y toujours un certain delta entre le dit et le fait, avec pour exemple les 8 % des administrations censées publier leurs données à le réaliser.

Mais l’interopérabilité passe également par les interfaces (API) développées afin de faciliter l’accès aux contenus web, avec des fonctionnalités comme la négociation de contenu.

Et si « contenu » implique « format », « format » fait réagir le data scientist, et même l’étudiant en data science. Comme toute activité qui s’appuie sur la récupération de données, la data science nécessite de les formater, et souvent de les nettoyer. Nos quelques années passées à extraire et à raffiner des données nous auront au moins appris ça. C’est pour cette raison, entre autres, qu’un data scientist devrait avoir l’interopérabilité du web à cœur, ne serait-ce que pour lui épargner des maux de tête.

Prenons l’exemple de Twitter : son API permet de récupérer des tweets en cherchant par mot-clé, par hashtag et plein d’autres critères. Le format retourné est du JSON propre et facile à réutiliser, et l’API est bien documentée (on ne suppose ici en aucun cas que cette facilité d’utilisation est due à la bienveillance et la clémence de Twitter, il s’agit simplement de comparer ce cas avec le second). Si l’on considère maintenant l’API de Facebook, on s’aperçoit que certaines fonctionnalités, comme la recherche par mot-clé, ont été dépréciées au cours des années. L’auteur s’est alors retrouvé dans ce qu’il appellera poliment « une impasse plus qu’indésirée ».

Dans ce contexte où les volumes de données sont de plus en plus importants, de même que les attentes client, le raffinage de données est de loin l’étape la plus chronophage du data scientist. Pour ma part, je m’intéresse plus particulièrement au text mining, Natural Language Processing et Natural Language Generation, qui s’appuient sur des données textuelles. En l’état de mes connaissances et expériences, l’encodage est la bête noire de ces activités. Une interopérabilité augmentée, peut-être au moyen d’une norme globale (travail déjà en cours il me semble, mené par une équipe de européenne), serait la bienvenue.